{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "The data source is a REDCap audit logging file, a REDCap report, and a CSV extract from Clarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import shutil \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 75)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "\n",
    "# import most recent logging file from REDCap (REDCap > Sidebar > Logging > Export all logging (CSV))\n",
    "latest_log_filepath = max(glob.iglob('data/raw/UCI REDCap Log/*.csv'), key=os.path.getmtime)\n",
    "log = pd.read_csv(latest_log_filepath)\n",
    "\n",
    "# import most recent extract aka \"report\" of useful data from REDCap (REDCap > Sidebar > Data Exports, Reports, and Stats)\n",
    "latest_redcap_extract_filepath = max(glob.iglob('data/raw/UCI REDCap Report/*.csv'), key=os.path.getmtime)\n",
    "redcap_extract = pd.read_csv(latest_redcap_extract_filepath)\n",
    "\n",
    "# copy to local storage most recent extract used for intervention tracking from Clarity \n",
    "latest_clarity_extract_filepath = max(glob.iglob('Z:/PCORI ACP Lists/*.csv'), key=os.path.getmtime)\n",
    "copy_clarity_extract_filepath = ('data/raw/Clarity Extract/')\n",
    "shutil.copy(latest_clarity_extract_filepath, copy_clarity_extract_filepath)\n",
    "\n",
    "# import copy of most recent extract used for intervention tracking from Clarity\n",
    "latest_copy_clarity_extract_filepath = max(glob.iglob('data/raw/Clarity Extract/*.csv'), key=os.path.getmtime)\n",
    "clarity_extract = pd.read_csv(latest_copy_clarity_extract_filepath)\n",
    "\n",
    "# import mailing lists for 2nd round, duplicate baseline research surveys\n",
    "round_2_mailing_a = pd.read_csv('data/raw/Baseline Survey Round 2 Mailing Lists/2.5.20_mailing_list_A.csv')\n",
    "round_2_mailing_b = pd.read_csv('data/raw/Baseline Survey Round 2 Mailing Lists/2.5.20_mailing_list_B.csv')\n",
    "round_2_mailing_c = pd.read_csv('data/raw/Baseline Survey Round 2 Mailing Lists/2.5.20_mailing_list_C.csv')\n",
    "round_2_mailing_d = pd.read_csv('data/raw/Baseline Survey Round 2 Mailing Lists/2.5.20_mailing_list_D.csv')\n",
    "\n",
    "# import most recent list of pop cohort study ID's with geocoded data (lat & long coordinates, Census Tract codes)\n",
    "latest_pop_geoID_filepath = max(glob.iglob(\"C:/Users/chauaj1/Documents/Python Projects/Geocoding/data/processed/4-final_output/*.csv\"), key=os.path.getmtime)\n",
    "pop_geoID = pd.read_csv(latest_pop_geoID_filepath)\n",
    "\n",
    "# import CDC's 2018 CSV file of California's Census Tracts + SVI \n",
    "# https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html\n",
    "SVI_source = pd.read_csv('data/raw/CDC 2018 SVI/California.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Log: Split into round 1 / 2 / 3 call dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Round 1 Call Updates:  790\n",
      "# of Round 2 Call Updates:  509\n",
      "# of Round 3 Call Updates:  287\n",
      "\n",
      "Total Call Updates:  1586\n"
     ]
    }
   ],
   "source": [
    "# make a new df with only REDCap Patient Record Updates\n",
    "log_updates = log[log['Action'].str.contains(\"Updated\")].copy()\n",
    "\n",
    "# make a new df with only REDCap Patient Record Updates + Phone Call 1 / 2 / 3 Attempts\n",
    "# assumes that RA updating the contact date field in Internal REDCAp = call completed\n",
    "log_updates_call1 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact1_dt\", na=False)].copy()\n",
    "log_updates_call2 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact2_dt\", na=False)].copy()\n",
    "log_updates_call3 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact3_dt\", na=False)].copy()\n",
    "\n",
    "print(\"# of Round 1 Call Updates: \", log_updates_call1.shape[0])\n",
    "print(\"# of Round 2 Call Updates: \", log_updates_call2.shape[0])\n",
    "print(\"# of Round 3 Call Updates: \", log_updates_call3.shape[0])\n",
    "print(\"\\nTotal Call Updates: \", log_updates_call1.shape[0] + log_updates_call2.shape[0] + log_updates_call3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Log: obtain study ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract study ID from long string ('Action')\n",
    "log_updates_call1['study_id'] = log_updates_call1['Action'].str.split(' ').str[2]\n",
    "log_updates_call2['study_id'] = log_updates_call2['Action'].str.split(' ').str[2]\n",
    "log_updates_call3['study_id'] = log_updates_call3['Action'].str.split(' ').str[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Log: handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original round 1 duplicates:  0\n",
      "original round 2 duplicates:  7\n",
      "original round 3 duplicates:  2\n",
      "\n",
      "new round 1 duplicates:  0\n",
      "new round 2 duplicates:  0\n",
      "new round 3 duplicates:  0\n",
      "\n",
      "# of Round 1 Calls:  790\n",
      "# of Round 2 Calls:  505\n",
      "# of Round 3 Calls:  286\n",
      "\n",
      "Total Calls:  1581\n"
     ]
    }
   ],
   "source": [
    "# convert dtype from string to datetime\n",
    "log_updates_call1['Time / Date'] = pd.to_datetime(log_updates_call1['Time / Date'])\n",
    "log_updates_call2['Time / Date'] = pd.to_datetime(log_updates_call2['Time / Date'])\n",
    "log_updates_call3['Time / Date'] = pd.to_datetime(log_updates_call3['Time / Date'])\n",
    "\n",
    "# sort calls by date from oldest to newest\n",
    "log_updates_call1 = log_updates_call1.sort_values(by=['Time / Date'])\n",
    "log_updates_call2 = log_updates_call2.sort_values(by=['Time / Date'])\n",
    "log_updates_call3 = log_updates_call3.sort_values(by=['Time / Date'])\n",
    "\n",
    "# check for duplicate updates to 1 study ID\n",
    "print(\"original round 1 duplicates: \", log_updates_call1[log_updates_call1['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"original round 2 duplicates: \", log_updates_call2[log_updates_call2['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"original round 3 duplicates: \", log_updates_call3[log_updates_call3['study_id'].duplicated(keep=False)].shape[0])\n",
    "\n",
    "# when there are multiple updates to 1 study ID, only take the most recent update\n",
    "unique_log_updates_call1 = log_updates_call1.drop_duplicates(subset=['study_id'], keep='last')\n",
    "unique_log_updates_call2 = log_updates_call2.drop_duplicates(subset=['study_id'], keep='last')\n",
    "unique_log_updates_call3 = log_updates_call3.drop_duplicates(subset=['study_id'], keep='last')\n",
    "\n",
    "# check for duplicate updates to 1 study ID\n",
    "print(\"\\nnew round 1 duplicates: \", unique_log_updates_call1[unique_log_updates_call1['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"new round 2 duplicates: \", unique_log_updates_call2[unique_log_updates_call2['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"new round 3 duplicates: \", unique_log_updates_call3[unique_log_updates_call3['study_id'].duplicated(keep=False)].shape[0])\n",
    "\n",
    "print(\"\\n# of Round 1 Calls: \", unique_log_updates_call1.shape[0])\n",
    "print(\"# of Round 2 Calls: \", unique_log_updates_call2.shape[0])\n",
    "print(\"# of Round 3 Calls: \", unique_log_updates_call3.shape[0])\n",
    "print(\"\\nTotal Calls: \", unique_log_updates_call1.shape[0] + unique_log_updates_call2.shape[0] + unique_log_updates_call3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Log: extract useful data from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names:  ['study_id', 'caller_username_1', 'call_timestamp_1', 'call_date_1', 'call_output_1', 'call_notes_1', 'call_verbal_consent_1']\n",
      "shape of round 1 dataset:  (790, 7)\n",
      "shape of round 2 dataset:  (505, 7)\n",
      "shape of round 3 dataset:  (286, 7)\n"
     ]
    }
   ],
   "source": [
    "# extract individual updates from long string ('List of Data Changes OR Fields Exported'), based on delimiter ','\n",
    "split_unique_log_updates_call1 = pd.concat([unique_log_updates_call1['study_id'], \n",
    "                                     unique_log_updates_call1['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "split_unique_log_updates_call2 = pd.concat([unique_log_updates_call2['study_id'],\n",
    "                                     unique_log_updates_call2['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "split_unique_log_updates_call3 = pd.concat([unique_log_updates_call3['study_id'],\n",
    "                                     unique_log_updates_call3['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "\n",
    "\n",
    "\n",
    "# extract updates specific to round 1 / 2 / 3 phone calls \n",
    "# make a boolean mask of all columns in the round 1 / 2 / 3 dataframe, True for matching strings\n",
    "# forward fill rows with matching strings and take only the last value \n",
    "# basically picks out the matching value regardless of column location and places it into the correct column\n",
    "\n",
    "# round 1\n",
    "df = split_unique_log_updates_call1.copy()\n",
    "strings = ['contact1_dt', 'contact1_output', 'contact1_nt', 'verbal_yn']\n",
    "updates_round_1 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_1[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call1.copy()\n",
    "\n",
    "# extracts date from 'contact1_dt' output\n",
    "updates_round_1.iloc[:,0] = pd.to_datetime(updates_round_1.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "# convert call code outputs to real words, per UCI's REDCap Codebook \n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_1.iloc[:,3] = updates_round_1.iloc[:,3].replace({\" verbal_yn = '0'\": \"No\"}, regex=True)\n",
    "updates_round_1.iloc[:,3] = updates_round_1.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes\"}, regex=True)\n",
    "    \n",
    "    \n",
    "# round 2\n",
    "df = split_unique_log_updates_call2.copy()\n",
    "strings = ['contact2_dt', 'contact2_output', 'contact2_nt', 'verbal_yn']\n",
    "updates_round_2 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_2[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call2.copy()\n",
    "\n",
    "updates_round_2.iloc[:,0] = pd.to_datetime(updates_round_2.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_2.iloc[:,3] = updates_round_2.iloc[:,3].replace({\" verbal_yn = '0'\": \"No\"}, regex=True)\n",
    "updates_round_2.iloc[:,3] = updates_round_2.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes\"}, regex=True)\n",
    "\n",
    "\n",
    "# round 3\n",
    "df = split_unique_log_updates_call3.copy()\n",
    "strings = ['contact3_dt', 'contact3_output', 'contact3_nt', 'verbal_yn']\n",
    "updates_round_3 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_3[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call3.copy()\n",
    "\n",
    "updates_round_3.iloc[:,0] = pd.to_datetime(updates_round_3.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_3.iloc[:,3] = updates_round_3.iloc[:,3].replace({\" verbal_yn = '0'\": \"No\"}, regex=True)\n",
    "updates_round_3.iloc[:,3] = updates_round_3.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes\"}, regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create new dataframes with relevant columns for round 1 / 2 / 3 calls \n",
    "\n",
    "# round 1\n",
    "clean_round_1 = split_unique_log_updates_call1['study_id'].copy()\n",
    "clean_round_1 = pd.concat((clean_round_1, unique_log_updates_call1['Username']), axis=1)\n",
    "clean_round_1 = pd.concat((clean_round_1, unique_log_updates_call1['Time / Date']), axis=1)\n",
    "clean_round_1 = pd.concat((clean_round_1, updates_round_1), axis=1)\n",
    "clean_round_1.columns = ['study_id', 'caller_username_1', 'call_timestamp_1', 'call_date_1', 'call_output_1', 'call_notes_1', 'call_verbal_consent_1']\n",
    "\n",
    "\n",
    "# round 2 \n",
    "clean_round_2 = split_unique_log_updates_call2['study_id'].copy()\n",
    "clean_round_2 = pd.concat((clean_round_2, unique_log_updates_call2['Username']), axis=1)\n",
    "clean_round_2 = pd.concat((clean_round_2, unique_log_updates_call2['Time / Date']), axis=1)\n",
    "clean_round_2 = pd.concat((clean_round_2, updates_round_2), axis=1)\n",
    "clean_round_2.columns = ['study_id', 'caller_username_2', 'call_timestamp_2', 'call_date_2', 'call_output_2', 'call_notes_2', 'call_verbal_consent_2']\n",
    "\n",
    "\n",
    "# round 3\n",
    "clean_round_3 = split_unique_log_updates_call3['study_id'].copy()\n",
    "clean_round_3 = pd.concat((clean_round_3, unique_log_updates_call3['Username']), axis=1)\n",
    "clean_round_3 = pd.concat((clean_round_3, unique_log_updates_call3['Time / Date']), axis=1)\n",
    "clean_round_3 = pd.concat((clean_round_3, updates_round_3), axis=1)\n",
    "clean_round_3.columns = ['study_id', 'caller_username_3', 'call_timestamp_3', 'call_date_3', 'call_output_3', 'call_notes_3', 'call_verbal_consent_3']\n",
    "\n",
    "\n",
    "print(\"column names: \", clean_round_1.columns.values.tolist())\n",
    "print(\"shape of round 1 dataset: \", clean_round_1.shape)\n",
    "print(\"shape of round 2 dataset: \", clean_round_2.shape)\n",
    "print(\"shape of round 3 dataset: \", clean_round_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Log: (UCI ONLY) shift incorrect datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI-Only Correction: Timestamps created during 2/3 - 2/5 are 8 hours ahead, due to REDCap upgrade error\n",
    "# incorrect updates range from to 02/03/2020 5:30pm to 02/06/2020 1:50am\n",
    "# only the timestamps that come from the REDCap audit log are affected, not the \"call_date\" which is input by users\n",
    "# store indeces of calls made between incorrect range\n",
    "# change timestamps of incorrect calls to shift 8 hrs earlier\n",
    "round_1_mistakes_index = clean_round_1[(clean_round_1['call_timestamp_1'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_1['call_timestamp_1'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_1.loc[round_1_mistakes_index, 'call_timestamp_1'] += pd.DateOffset(hours=-8)\n",
    "\n",
    "\n",
    "round_2_mistakes_index = clean_round_2[(clean_round_2['call_timestamp_2'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_2['call_timestamp_2'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_2.loc[round_2_mistakes_index, 'call_timestamp_2'] += pd.DateOffset(hours=-8)\n",
    "\n",
    "\n",
    "round_3_mistakes_index = clean_round_3[(clean_round_3['call_timestamp_3'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_3['call_timestamp_3'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_3.loc[round_3_mistakes_index, 'call_timestamp_3'] += pd.DateOffset(hours=-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDCap Report: convert number coding to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id                             int64\n",
      "redcap_event_name                   object\n",
      "survey_language_sent                 int64\n",
      "hipaa_sent_yn                        int64\n",
      "survey_completed_dt                 object\n",
      "consent_received_dt                 object\n",
      "survery_completed_method           float64\n",
      "hipaa_received_dt                   object\n",
      "caregiver_name                      object\n",
      "consent_mailed_dt                   object\n",
      "gift_card_type                     float64\n",
      "opt_out_pat_dt                      object\n",
      "opted_out_patient_reasons          float64\n",
      "opted_out_patient_other             object\n",
      "opted_out_patient_transcription     object\n",
      "research_cohort_yn                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(redcap_extract.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for some reason, some columns are stored as floats, which is problematic when converting to strings \n",
    "\n",
    "# convert the float columns to Int64 (which allows null values to coexist with ints)\n",
    "redcap_extract['survery_completed_method'] = redcap_extract['survery_completed_method'].astype('Int64')\n",
    "redcap_extract['gift_card_type'] = redcap_extract['gift_card_type'].astype('Int64')\n",
    "redcap_extract['opted_out_patient_reasons'] = redcap_extract['opted_out_patient_reasons'].astype('Int64')\n",
    "redcap_extract['research_cohort_yn'] = redcap_extract['research_cohort_yn'].astype('Int64')\n",
    "\n",
    "# convert entire dataframe to strings\n",
    "redcap_extract = redcap_extract.applymap(str)\n",
    "\n",
    "# convert dates back to datetime data type\n",
    "redcap_extract['survey_completed_dt'] = pd.to_datetime(redcap_extract['survey_completed_dt'])\n",
    "redcap_extract['consent_received_dt'] = pd.to_datetime(redcap_extract['consent_received_dt'])\n",
    "redcap_extract['hipaa_received_dt'] = pd.to_datetime(redcap_extract['hipaa_received_dt'])\n",
    "redcap_extract['consent_mailed_dt'] = pd.to_datetime(redcap_extract['consent_mailed_dt'])\n",
    "\n",
    "redcap_extract.loc[redcap_extract['survey_language_sent'] == \"0\", 'survey_language_sent'] = \"English\"\n",
    "redcap_extract.loc[redcap_extract['survey_language_sent'] == \"1\", 'survey_language_sent'] = \"Spanish\"\n",
    "\n",
    "redcap_extract.loc[redcap_extract['hipaa_sent_yn'] == \"0\", 'hipaa_sent_yn'] = \"No\"\n",
    "redcap_extract.loc[redcap_extract['hipaa_sent_yn'] == \"1\", 'hipaa_sent_yn'] = \"Yes\"\n",
    "\n",
    "redcap_extract.loc[redcap_extract['survery_completed_method'] == \"1\", 'survery_completed_method'] = \"Paper\"\n",
    "redcap_extract.loc[redcap_extract['survery_completed_method'] == \"2\", 'survery_completed_method'] = \"Phone\"\n",
    "redcap_extract.loc[redcap_extract['survery_completed_method'] == \"3\", 'survery_completed_method'] = \"Email\"\n",
    "\n",
    "redcap_extract.loc[redcap_extract['gift_card_type'] == \"1\", 'gift_card_type'] = \"e-Gift Card\"\n",
    "redcap_extract.loc[redcap_extract['gift_card_type'] == \"2\", 'gift_card_type'] = \"Physical Gift Card\"\n",
    "redcap_extract.loc[redcap_extract['gift_card_type'] == \"3\", 'gift_card_type'] = \"Target Gift Card\"\n",
    "redcap_extract.loc[redcap_extract['gift_card_type'] == \"4\", 'gift_card_type'] = \"Patient Declined Gift Card\"\n",
    "\n",
    "redcap_extract.loc[redcap_extract['research_cohort_yn'] == '1', 'research_cohort_yn'] = \"Yes\"\n",
    "\n",
    "# add date that 1st bulk mailing was dropped off at USPS\n",
    "redcap_extract['survey_mailing_date_1'] = pd.to_datetime('11/13/2019')\n",
    "\n",
    "# NOTE: may want to add some additional items from Internal REDCap\n",
    "useful_redcap_cols = ['study_id', \n",
    "                      'survey_language_sent', \n",
    "                      'hipaa_sent_yn', \n",
    "                      'survey_completed_dt', \n",
    "                      'consent_received_dt', \n",
    "                      'survery_completed_method',  \n",
    "                      'hipaa_received_dt', \n",
    "                      'consent_mailed_dt',  \n",
    "                      'opt_out_pat_dt',\n",
    "                      'opted_out_patient_reasons', \n",
    "                      'opted_out_patient_other',\n",
    "                      'opted_out_patient_transcription',     # transcription of opt-out voicemail that patient left \n",
    "                      'survey_mailing_date_1',\n",
    "                      'research_cohort_yn']\n",
    "\n",
    "clean_redcap_extract = redcap_extract.filter(useful_redcap_cols).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Survey Round 2 Mailing Lists: merge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the 4 mailing lists sent to mail vendor and combine \n",
    "all_mailings = [round_2_mailing_a, round_2_mailing_b, round_2_mailing_c, round_2_mailing_d]\n",
    "all_round_2_mailings = pd.concat(all_mailings)\n",
    "\n",
    "# add date that 2nd bulk mailing was dropped off at USPS \n",
    "all_round_2_mailings['survey_mailing_date_2'] = pd.to_datetime('2/18/2020')\n",
    "\n",
    "# only take necessary columns\n",
    "useful_round_2_mailing_cols = ['study_id', 'survey_mailing_date_2']\n",
    "clean_round_2_mailings = all_round_2_mailings.filter(useful_round_2_mailing_cols).copy()\n",
    "\n",
    "# convert study_id to string\n",
    "clean_round_2_mailings = clean_round_2_mailings.astype({'study_id': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarity Extract: calculate age, Orange Dot status, and convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to datetime data type\n",
    "clarity_extract['birth_date'] = pd.to_datetime(clarity_extract['birth_date'])\n",
    "clarity_extract['death_date'] = pd.to_datetime(clarity_extract['death_date'])\n",
    "clarity_extract['enroll_date'] = pd.to_datetime(clarity_extract['enroll_date'])\n",
    "clarity_extract['date_orange'] = pd.to_datetime(clarity_extract['date_orange'])\n",
    "\n",
    "# convert int to string for consistency\n",
    "clarity_extract['study_id'] = clarity_extract['study_id'].astype(str)\n",
    "\n",
    "# assign \"yes\" to Orange Dot if the patient has an orange dot date, leave null for the rest of pts\n",
    "# NOTE: may not want to assign \"no\" because patients may not all have qualified to be in the orange dot (i.e. newly identified)\n",
    "clarity_extract.loc[clarity_extract['date_orange'].notnull(), 'orange_dot_yn'] = \"Yes\"\n",
    "\n",
    "\n",
    "# calculate age from today's date, unless patient is dead (then use deceased date) \n",
    "def calculate_age_current_or_death(date_of_birth, date_of_death): \n",
    "    today = date.today()\n",
    "    if date_of_death is pd.NaT: \n",
    "        return today.year - date_of_birth.year\n",
    "    else: \n",
    "        return date_of_death.year - date_of_birth.year\n",
    "\n",
    "clarity_extract['age_current_or_death'] = clarity_extract.apply(lambda x: calculate_age_current_or_death(x['birth_date'], x['death_date']), axis=1)\n",
    "\n",
    "\n",
    "# calculate age from orange dot date or enrollment date if they were added after the orange dot date\n",
    "def calculate_age_orange_dot_or_enrollment(date_of_birth, orange_dot_date, enrollment_date): \n",
    "    if orange_dot_date is pd.NaT: \n",
    "        return enrollment_date.year - date_of_birth.year\n",
    "    else: \n",
    "        return orange_dot_date.year - date_of_birth.year\n",
    "\n",
    "clarity_extract['age_orange_dot_or_enrollment'] = clarity_extract.apply(lambda x: calculate_age_orange_dot_or_enrollment(x['birth_date'], x['date_orange'], x['enroll_date']), axis=1)\n",
    "\n",
    "\n",
    "# aggregate all patients over the age of 89 into a group with age 90\n",
    "clarity_extract.loc[clarity_extract['age_current_or_death'] > 89, 'age_current_or_death'] = 90\n",
    "clarity_extract.loc[clarity_extract['age_orange_dot_or_enrollment'] > 89, 'age_orange_dot_or_enrollment'] = 90\n",
    "\n",
    "# for consistency, make all \"no\" values --> null values\n",
    "#clarity_extract.loc[clarity_extract['research_cohort_yn'] == \"Y\", 'research_cohort_yn'] = \"Yes\"\n",
    "#clarity_extract.loc[clarity_extract['research_cohort_yn'] == \"N\", 'research_cohort_yn'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarity Extract: (UCI ONLY) exclude pre-Orange Dot patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The UCI Serious Illness code was run weekly periodically starting in September 2019. The UCI intervention did not\n",
    "#       go live until April 2020. During this 7 month time period, a number of the seriously ill patients expired. Therefore,\n",
    "#       they were not included in the Orange Dot. Any patient that was enrolled before the Orange Dot date (4/13/2020) and \n",
    "#       was not included in the Orange Dot cohort should be excluded. \n",
    "\n",
    "# make df of all patients who are not in the orange dot cohort\n",
    "non_OD_enroll_dates = clarity_extract[['study_id', 'enroll_date']][clarity_extract['orange_dot_yn'] != \"Yes\"].copy()\n",
    "\n",
    "# exclude all patients who are not in the orange dot cohort and were enrolled before the orange dot\n",
    "to_exclude = non_OD_enroll_dates[non_OD_enroll_dates['enroll_date'] < pd.to_datetime('4/13/2020')]\n",
    "trimmed_clarity_extract = clarity_extract[~clarity_extract['study_id'].isin(to_exclude['study_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarity Extract: filter and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select useful columns from Clarity extract\n",
    "# TODO: uncomment data elements when they are added to UCI extract\n",
    "useful_clarity_cols = ['study_id', \n",
    "                       'orange_dot_yn',\n",
    "                       'enroll_date',\n",
    "                       #'research_cohort_yn',   # not available from UCI extract as of 2/17/21\n",
    "                       'arm',                   # current arm assignment \n",
    "                       'department_name',       # current clinic\n",
    "                       'mct_status',            # MyChart status \n",
    "                       'age_current_or_death',\n",
    "                       'age_orange_dot_or_enrollment',\n",
    "                       'sex', \n",
    "                       'race', \n",
    "                       'ethnicity',\n",
    "                       'spoken_language', \n",
    "                       'written_language',\n",
    "                       #'insurance',            # not available from UCI extract yet\n",
    "                       #'religion',             # not available from UCI extract yet\n",
    "                       #'illness_adv_cancer',   # not available from UCI extract yet \n",
    "                       #'illness_ESLD',         # not available from UCI extract yet\n",
    "                       #'illness_COPD',         # not available from UCI extract yet\n",
    "                       #'illness_CHF',          # not available from UCI extract yet\n",
    "                       #'illness_ESRD'          # not available from UCI extract yet\n",
    "                       #'birth_date',\n",
    "                       #'death_date'\n",
    "                       ]\n",
    "\n",
    "clean_trimmed_clarity_extract = trimmed_clarity_extract.filter(useful_clarity_cols).copy()\n",
    "\n",
    "# rename columns for consistency  \n",
    "clean_trimmed_clarity_extract.columns = ['study_id', \n",
    "                                         'orange_dot_yn',\n",
    "                                         'enroll_date',\n",
    "                                         #'research_cohort_yn',\n",
    "                                         'arm',\n",
    "                                         'clinic', \n",
    "                                         'mychart_status',\n",
    "                                         'age_current_or_death',\n",
    "                                         'age_orange_dot_or_enrollment',\n",
    "                                         'sex', \n",
    "                                         'race', \n",
    "                                         'ethnicity',\n",
    "                                         'spoken_language', \n",
    "                                         'written_language',\n",
    "                                         #'birth_date',\n",
    "                                         #'death_date'\n",
    "                                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Cohort GeoID: merge on Census tract codes to find SVI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "useful_SVI_cols = ['FIPS',          # concatenation of geocoding IDs, including Census Tract code\n",
    "                   'RPL_THEME1',    # Javi: percentile ranking for socioeconomic theme\n",
    "                   'RPL_THEME2',    # Javi: percentile ranking for household composition theme\n",
    "                   'RPL_THEME3',    # Javi: percentile ranking for minority status and language theme\n",
    "                   'RPL_THEME4',    # Javi: percentile ranking for housing type and transportation composition theme\n",
    "                   'RPL_THEMES']    # Javi: overall percentile ranking for SVI\n",
    "\n",
    "clean_SVI_source = SVI_source.filter(useful_SVI_cols).copy()\n",
    "\n",
    "clean_SVI_source.columns = ['FIPS',\n",
    "                            'SVI_socioeconomic',\n",
    "                            'SVI_household_comp',\n",
    "                            'SVI_minority_language',\n",
    "                            'SVI_housing_transportation',\n",
    "                            'SVI_total']\n",
    "\n",
    "\n",
    "# NOTE: for some reason, removing the 1st and last number from the Census \"GEOID\" value = the CDC's \"FIPS\" value\n",
    "pop_geoID['FIPS'] = pop_geoID['GEOID'].str[1:-1]\n",
    "\n",
    "# convert CDC's \"FIPS\" int value to match Census's \"GEOID\" string value\n",
    "clean_SVI_source['FIPS'] = clean_SVI_source['FIPS'].astype(str)\n",
    "\n",
    "pop_geoID_SVI = pd.merge(pop_geoID, clean_SVI_source, how='left', on='FIPS')\n",
    "\n",
    "pop_geoID_SVI['study_id'] = pop_geoID_SVI['study_id'].astype(str)\n",
    "\n",
    "useful_pop_geoID_SVI_col = ['study_id',\n",
    "                            'SVI_socioeconomic',\n",
    "                            'SVI_household_comp',\n",
    "                            'SVI_minority_language',\n",
    "                            'SVI_housing_transportation',\n",
    "                            'SVI_total']\n",
    "\n",
    "clean_pop_geoID_SVI = pop_geoID_SVI.filter(useful_pop_geoID_SVI_col).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join REDCap Extract, REDCap Log, Geocoded Data, and Clarity Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1798, 50)\n"
     ]
    }
   ],
   "source": [
    "# merge round 1 / 2 / 3 calls so there is 1 patient per line with all call data in columns \n",
    "clean_1_2_calls = pd.merge(clean_round_1, clean_round_2, how='left', on='study_id')\n",
    "clean_all_calls = pd.merge(clean_1_2_calls, clean_round_3, how='left', on='study_id')\n",
    "\n",
    "# join patients who were called with patients who returned surveys (source: REDCap report / extract)\n",
    "calls_and_redcap = pd.merge(clean_all_calls, clean_redcap_extract, how='outer', on='study_id')\n",
    "\n",
    "# add 2nd round mailing dates\n",
    "calls_and_redcap_and_mailing = pd.merge(calls_and_redcap, clean_round_2_mailings, how='left', on='study_id')\n",
    "\n",
    "# include all other pts in population cohort and add demographic data \n",
    "pop_cohort = pd.merge(calls_and_redcap_and_mailing, clean_trimmed_clarity_extract, how='outer', on='study_id')\n",
    "\n",
    "# add SVI data \n",
    "full_pop_cohort = pd.merge(pop_cohort, clean_pop_geoID_SVI, how='left', on='study_id')\n",
    "\n",
    "print(full_pop_cohort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id                                   object\n",
      "caller_username_1                          object\n",
      "call_timestamp_1                   datetime64[ns]\n",
      "call_date_1                        datetime64[ns]\n",
      "call_output_1                              object\n",
      "call_notes_1                               object\n",
      "call_verbal_consent_1                      object\n",
      "caller_username_2                          object\n",
      "call_timestamp_2                   datetime64[ns]\n",
      "call_date_2                        datetime64[ns]\n",
      "call_output_2                              object\n",
      "call_notes_2                               object\n",
      "call_verbal_consent_2                      object\n",
      "caller_username_3                          object\n",
      "call_timestamp_3                   datetime64[ns]\n",
      "call_date_3                        datetime64[ns]\n",
      "call_output_3                              object\n",
      "call_notes_3                               object\n",
      "call_verbal_consent_3                      object\n",
      "survey_language_sent                       object\n",
      "hipaa_sent_yn                              object\n",
      "survey_completed_dt                datetime64[ns]\n",
      "consent_received_dt                datetime64[ns]\n",
      "survery_completed_method                   object\n",
      "hipaa_received_dt                  datetime64[ns]\n",
      "consent_mailed_dt                  datetime64[ns]\n",
      "opt_out_pat_dt                             object\n",
      "opted_out_patient_reasons                  object\n",
      "opted_out_patient_other                    object\n",
      "opted_out_patient_transcription            object\n",
      "survey_mailing_date_1              datetime64[ns]\n",
      "research_cohort_yn                         object\n",
      "survey_mailing_date_2              datetime64[ns]\n",
      "orange_dot_yn                              object\n",
      "enroll_date                        datetime64[ns]\n",
      "arm                                       float64\n",
      "clinic                                     object\n",
      "mychart_status                             object\n",
      "age_current_or_death                      float64\n",
      "age_orange_dot_or_enrollment              float64\n",
      "sex                                        object\n",
      "race                                       object\n",
      "ethnicity                                  object\n",
      "spoken_language                            object\n",
      "written_language                           object\n",
      "SVI_socioeconomic                         float64\n",
      "SVI_household_comp                        float64\n",
      "SVI_minority_language                     float64\n",
      "SVI_housing_transportation                float64\n",
      "SVI_total                                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(full_pop_cohort.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_cohort.to_csv('data/processed/UCI_recruitment_cohort2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
