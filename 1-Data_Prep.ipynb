{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "The data source is a REDCap audit logging file, a REDCap report, and a CSV extract from Clarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 75\n",
    "\n",
    "# import logging file from REDCap (REDCap > Sidebar > Logging > Export all logging (CSV))\n",
    "log = pd.read_csv('data/raw/UCIInternalREDCapPCORIProject_Logging_2021-01-13_2027.csv')\n",
    "\n",
    "# import extract aka \"report\" of useful data from REDCap (REDCap > Sidebar > Data Exports, Reports, and Stats)\n",
    "redcap_extract = pd.read_csv('data/raw/UCIInternalREDCapPCO-AdminExportForRecrui_DATA_2021-01-21_1832.csv')\n",
    "\n",
    "# import extract used for intervention tracking from Clarity\n",
    "clarity_extract = pd.read_csv('data/raw/20201229_PCORIWeeklyExtract.csv')\n",
    "\n",
    "# import mailing lists for 2nd round, duplicate baseline research surveys\n",
    "round_2_mailing_a = pd.read_csv('data/raw/2.5.20_mailing_list_A.csv')\n",
    "round_2_mailing_b = pd.read_csv('data/raw/2.5.20_mailing_list_B.csv')\n",
    "round_2_mailing_c = pd.read_csv('data/raw/2.5.20_mailing_list_C.csv')\n",
    "round_2_mailing_d = pd.read_csv('data/raw/2.5.20_mailing_list_D.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Log: Split into round 1 / 2 / 3 call dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Round 1 Call Updates:  790\n",
      "# of Round 2 Call Updates:  509\n",
      "# of Round 3 Call Updates:  287\n",
      "\n",
      "Total Call Updates:  1586\n"
     ]
    }
   ],
   "source": [
    "# make a new df with only REDCap Patient Record Updates\n",
    "log_updates = log[log['Action'].str.contains(\"Updated\")].copy()\n",
    "\n",
    "# make a new df with only REDCap Patient Record Updates + Phone Call 1 / 2 / 3 Attempts\n",
    "# assumes that RA updating the contact date field in Internal REDCAp = call completed\n",
    "log_updates_call1 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact1_dt\", na=False)].copy()\n",
    "log_updates_call2 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact2_dt\", na=False)].copy()\n",
    "log_updates_call3 = log_updates[log_updates['List of Data Changes OR Fields Exported'].str.contains(\"contact3_dt\", na=False)].copy()\n",
    "\n",
    "print(\"# of Round 1 Call Updates: \", log_updates_call1.shape[0])\n",
    "print(\"# of Round 2 Call Updates: \", log_updates_call2.shape[0])\n",
    "print(\"# of Round 3 Call Updates: \", log_updates_call3.shape[0])\n",
    "print(\"\\nTotal Call Updates: \", log_updates_call1.shape[0] + log_updates_call2.shape[0] + log_updates_call3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log: obtain study ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract study ID from long string ('Action')\n",
    "log_updates_call1['study_id'] = log_updates_call1['Action'].str.split(' ').str[2]\n",
    "log_updates_call2['study_id'] = log_updates_call2['Action'].str.split(' ').str[2]\n",
    "log_updates_call3['study_id'] = log_updates_call3['Action'].str.split(' ').str[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log: handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original round 1 duplicates:  0\n",
      "original round 2 duplicates:  7\n",
      "original round 3 duplicates:  2\n",
      "\n",
      "new round 1 duplicates:  0\n",
      "new round 2 duplicates:  0\n",
      "new round 3 duplicates:  0\n",
      "\n",
      "# of Round 1 Calls:  790\n",
      "# of Round 2 Calls:  505\n",
      "# of Round 3 Calls:  286\n",
      "\n",
      "Total Calls:  1581\n"
     ]
    }
   ],
   "source": [
    "# convert dtype from string to datetime\n",
    "log_updates_call1['Time / Date'] = pd.to_datetime(log_updates_call1['Time / Date'])\n",
    "log_updates_call2['Time / Date'] = pd.to_datetime(log_updates_call2['Time / Date'])\n",
    "log_updates_call3['Time / Date'] = pd.to_datetime(log_updates_call3['Time / Date'])\n",
    "\n",
    "# sort calls by date from oldest to newest\n",
    "log_updates_call1 = log_updates_call1.sort_values(by=['Time / Date'])\n",
    "log_updates_call2 = log_updates_call2.sort_values(by=['Time / Date'])\n",
    "log_updates_call3 = log_updates_call3.sort_values(by=['Time / Date'])\n",
    "\n",
    "# check for duplicate updates to 1 study ID\n",
    "print(\"original round 1 duplicates: \", log_updates_call1[log_updates_call1['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"original round 2 duplicates: \", log_updates_call2[log_updates_call2['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"original round 3 duplicates: \", log_updates_call3[log_updates_call3['study_id'].duplicated(keep=False)].shape[0])\n",
    "\n",
    "# when there are multiple updates to 1 study ID, only take the most recent update\n",
    "unique_log_updates_call1 = log_updates_call1.drop_duplicates(subset=['study_id'], keep='last')\n",
    "unique_log_updates_call2 = log_updates_call2.drop_duplicates(subset=['study_id'], keep='last')\n",
    "unique_log_updates_call3 = log_updates_call3.drop_duplicates(subset=['study_id'], keep='last')\n",
    "\n",
    "# check for duplicate updates to 1 study ID\n",
    "print(\"\\nnew round 1 duplicates: \", unique_log_updates_call1[unique_log_updates_call1['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"new round 2 duplicates: \", unique_log_updates_call2[unique_log_updates_call2['study_id'].duplicated(keep=False)].shape[0])\n",
    "print(\"new round 3 duplicates: \", unique_log_updates_call3[unique_log_updates_call3['study_id'].duplicated(keep=False)].shape[0])\n",
    "\n",
    "print(\"\\n# of Round 1 Calls: \", unique_log_updates_call1.shape[0])\n",
    "print(\"# of Round 2 Calls: \", unique_log_updates_call2.shape[0])\n",
    "print(\"# of Round 3 Calls: \", unique_log_updates_call3.shape[0])\n",
    "print(\"\\nTotal Calls: \", unique_log_updates_call1.shape[0] + unique_log_updates_call2.shape[0] + unique_log_updates_call3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log: extract useful data from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names:  ['study_id', 'caller_username_1', 'call_timestamp_1', 'call_date_1', 'call_output_1', 'call_notes_1', 'call_verbal_consent_1']\n",
      "shape of round 1 dataset:  (790, 7)\n",
      "shape of round 2 dataset:  (505, 7)\n",
      "shape of round 3 dataset:  (286, 7)\n"
     ]
    }
   ],
   "source": [
    "# extract individual updates from long string ('List of Data Changes OR Fields Exported'), based on delimiter ','\n",
    "split_unique_log_updates_call1 = pd.concat([unique_log_updates_call1['study_id'], \n",
    "                                     unique_log_updates_call1['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "split_unique_log_updates_call2 = pd.concat([unique_log_updates_call2['study_id'],\n",
    "                                     unique_log_updates_call2['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "split_unique_log_updates_call3 = pd.concat([unique_log_updates_call3['study_id'],\n",
    "                                     unique_log_updates_call3['List of Data Changes OR Fields Exported'].str.split(',', expand=True)],\n",
    "                                    axis=1,)\n",
    "\n",
    "\n",
    "\n",
    "# extract updates specific to round 1 / 2 / 3 phone calls \n",
    "# make a boolean mask of all columns in the round 1 / 2 / 3 dataframe, True for matching strings\n",
    "# forward fill rows with matching strings and take only the last value \n",
    "# basically picks out the matching value regardless of column location and places it into the correct column\n",
    "\n",
    "# round 1\n",
    "df = split_unique_log_updates_call1.copy()\n",
    "strings = ['contact1_dt', 'contact1_output', 'contact1_nt', 'verbal_yn']\n",
    "updates_round_1 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_1[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call1.copy()\n",
    "\n",
    "# extracts date from 'contact1_dt' output\n",
    "updates_round_1.iloc[:,0] = pd.to_datetime(updates_round_1.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "# convert call code outputs to real words, per UCI's REDCap Codebook \n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_1.iloc[:,1] = updates_round_1.iloc[:,1].replace({\" contact1_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_1.iloc[:,3] = updates_round_1.iloc[:,3].replace({\" verbal_yn = '0'\": \"No Verbal Consent\"}, regex=True)\n",
    "updates_round_1.iloc[:,3] = updates_round_1.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes Verbal Consent\"}, regex=True)\n",
    "    \n",
    "    \n",
    "# round 2\n",
    "df = split_unique_log_updates_call2.copy()\n",
    "strings = ['contact2_dt', 'contact2_output', 'contact2_nt', 'verbal_yn']\n",
    "updates_round_2 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_2[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call2.copy()\n",
    "\n",
    "updates_round_2.iloc[:,0] = pd.to_datetime(updates_round_2.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_2.iloc[:,1] = updates_round_2.iloc[:,1].replace({\" contact2_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_2.iloc[:,3] = updates_round_2.iloc[:,3].replace({\" verbal_yn = '0'\": \"No Verbal Consent\"}, regex=True)\n",
    "updates_round_2.iloc[:,3] = updates_round_2.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes Verbal Consent\"}, regex=True)\n",
    "\n",
    "\n",
    "# round 3\n",
    "df = split_unique_log_updates_call3.copy()\n",
    "strings = ['contact3_dt', 'contact3_output', 'contact3_nt', 'verbal_yn']\n",
    "updates_round_3 = pd.DataFrame()\n",
    "\n",
    "for s in strings:\n",
    "    for col in df: \n",
    "        df[col] = df[col].mask(~df[col].str.contains(s, na=False))\n",
    "    updates_round_3[s] = df.ffill(axis=1).iloc[:, -1]\n",
    "    df = split_unique_log_updates_call3.copy()\n",
    "\n",
    "updates_round_3.iloc[:,0] = pd.to_datetime(updates_round_3.iloc[:,0].str.extract('(\\d{1,4}-\\d{1,2}-\\d{1,2})')[0])\n",
    "\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '1'\": \"No answer/unable to leave VM/busy/disconnected\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '2'\": \"Left a message\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '3'\": \"Call back later\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '4'\": \"Hasn't received packet yet, call back in one week\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '5'\": \"Hasn't received packet and team needs to resend in the mail\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '6'\": \"Send link to the survey\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '7'\": \"Completed survey by phone\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '8'\": \"Patient refused\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '9'\": \"Deceased\"}, regex=True)\n",
    "updates_round_3.iloc[:,1] = updates_round_3.iloc[:,1].replace({\" contact3_output = '10'\": \"Other Notes\"}, regex=True)\n",
    "\n",
    "updates_round_3.iloc[:,3] = updates_round_3.iloc[:,3].replace({\" verbal_yn = '0'\": \"No Verbal Consent\"}, regex=True)\n",
    "updates_round_3.iloc[:,3] = updates_round_3.iloc[:,3].replace({\" verbal_yn = '1'\": \"Yes Verbal Consent\"}, regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create new dataframes with relevant columns for round 1 / 2 / 3 calls \n",
    "\n",
    "# round 1\n",
    "clean_round_1 = split_unique_log_updates_call1['study_id'].copy()\n",
    "clean_round_1 = pd.concat((clean_round_1, unique_log_updates_call1['Username']), axis=1)\n",
    "clean_round_1 = pd.concat((clean_round_1, unique_log_updates_call1['Time / Date']), axis=1)\n",
    "clean_round_1 = pd.concat((clean_round_1, updates_round_1), axis=1)\n",
    "clean_round_1.columns = ['study_id', 'caller_username_1', 'call_timestamp_1', 'call_date_1', 'call_output_1', 'call_notes_1', 'call_verbal_consent_1']\n",
    "\n",
    "\n",
    "# round 2 \n",
    "clean_round_2 = split_unique_log_updates_call2['study_id'].copy()\n",
    "clean_round_2 = pd.concat((clean_round_2, unique_log_updates_call2['Username']), axis=1)\n",
    "clean_round_2 = pd.concat((clean_round_2, unique_log_updates_call2['Time / Date']), axis=1)\n",
    "clean_round_2 = pd.concat((clean_round_2, updates_round_2), axis=1)\n",
    "clean_round_2.columns = ['study_id', 'caller_username_2', 'call_timestamp_2', 'call_date_2', 'call_output_2', 'call_notes_2', 'call_verbal_consent_2']\n",
    "\n",
    "\n",
    "# round 3\n",
    "clean_round_3 = split_unique_log_updates_call3['study_id'].copy()\n",
    "clean_round_3 = pd.concat((clean_round_3, unique_log_updates_call3['Username']), axis=1)\n",
    "clean_round_3 = pd.concat((clean_round_3, unique_log_updates_call3['Time / Date']), axis=1)\n",
    "clean_round_3 = pd.concat((clean_round_3, updates_round_3), axis=1)\n",
    "clean_round_3.columns = ['study_id', 'caller_username_3', 'call_timestamp_3', 'call_date_3', 'call_output_3', 'call_notes_3', 'call_verbal_consent_3']\n",
    "\n",
    "\n",
    "print(\"column names: \", clean_round_1.columns.values.tolist())\n",
    "print(\"shape of round 1 dataset: \", clean_round_1.shape)\n",
    "print(\"shape of round 2 dataset: \", clean_round_2.shape)\n",
    "print(\"shape of round 3 dataset: \", clean_round_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log: (UCI ONLY) shift incorrect datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI-Only Correction: Timestamps created during 2/3 - 2/5 are 8 hours ahead, due to REDCap upgrade error\n",
    "# incorrect updates range from to 02/03/2020 5:30pm to 02/06/2020 1:50am\n",
    "# only the timestamps that come from the REDCap audit log are affected, not the \"call_date\" which is input by users\n",
    "# store indeces of calls made between incorrect range\n",
    "# change timestamps of incorrect calls to shift 8 hrs earlier\n",
    "round_1_mistakes_index = clean_round_1[(clean_round_1['call_timestamp_1'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_1['call_timestamp_1'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_1.loc[round_1_mistakes_index, 'call_timestamp_1'] += pd.DateOffset(hours=-8)\n",
    "\n",
    "\n",
    "round_2_mistakes_index = clean_round_2[(clean_round_2['call_timestamp_2'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_2['call_timestamp_2'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_2.loc[round_2_mistakes_index, 'call_timestamp_2'] += pd.DateOffset(hours=-8)\n",
    "\n",
    "\n",
    "round_3_mistakes_index = clean_round_3[(clean_round_3['call_timestamp_3'] > '2020-02-03 16:30:00') &\n",
    "                                       (clean_round_3['call_timestamp_3'] < '2020-02-06 02:00:00')].index.values.tolist()\n",
    "clean_round_3.loc[round_3_mistakes_index, 'call_timestamp_3'] += pd.DateOffset(hours=-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDCap Extract: convert number coding to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for some reason, the columns \"gift_card_type\" and \"survey_completed_method\" are stored as floats,\n",
    "#       which is problematic when converting to strings \n",
    "# convert the 2 float columns to Int64 (which allows null values to coexist with ints)\n",
    "redcap_extract['gift_card_type'] = redcap_extract['gift_card_type'].astype('Int64')\n",
    "redcap_extract['survery_completed_method'] = redcap_extract['survery_completed_method'].astype('Int64')\n",
    "\n",
    "# convert entire dataframe to strings\n",
    "redcap_extract = redcap_extract.applymap(str)\n",
    "\n",
    "# convert dates back to datetime data type\n",
    "redcap_extract['survey_completed_dt'] = pd.to_datetime(redcap_extract['survey_completed_dt'])\n",
    "redcap_extract['consent_received_dt'] = pd.to_datetime(redcap_extract['consent_received_dt'])\n",
    "redcap_extract['hipaa_received_dt'] = pd.to_datetime(redcap_extract['hipaa_received_dt'])\n",
    "redcap_extract['consent_mailed_dt'] = pd.to_datetime(redcap_extract['consent_mailed_dt'])\n",
    "\n",
    "redcap_extract.iloc[:,2] = redcap_extract.iloc[:,2].replace({\"0\": \"English\"}, regex=True)\n",
    "redcap_extract.iloc[:,2] = redcap_extract.iloc[:,2].replace({\"1\": \"Spanish\"}, regex=True)\n",
    "\n",
    "redcap_extract.iloc[:,3] = redcap_extract.iloc[:,3].replace({\"0\": \"No HIPAA Sent\"}, regex=True)\n",
    "redcap_extract.iloc[:,3] = redcap_extract.iloc[:,3].replace({\"1\": \"Yes HIPAA Sent\"}, regex=True)\n",
    "\n",
    "redcap_extract.iloc[:,6] = redcap_extract.iloc[:,6].replace({\"1\": \"Paper\"}, regex=True)\n",
    "redcap_extract.iloc[:,6] = redcap_extract.iloc[:,6].replace({\"2\": \"Phone\"}, regex=True)\n",
    "redcap_extract.iloc[:,6] = redcap_extract.iloc[:,6].replace({\"3\": \"Email\"}, regex=True)\n",
    "\n",
    "redcap_extract.iloc[:,10] = redcap_extract.iloc[:,10].replace({\"1\": \"e-Gift Card\"}, regex=True)\n",
    "redcap_extract.iloc[:,10] = redcap_extract.iloc[:,10].replace({\"2\": \"Physical Gift Card\"}, regex=True)\n",
    "redcap_extract.iloc[:,10] = redcap_extract.iloc[:,10].replace({\"3\": \"Target Gift Card\"}, regex=True)\n",
    "redcap_extract.iloc[:,10] = redcap_extract.iloc[:,10].replace({\"4\": \"Patient Declined Gift Card\"}, regex=True)\n",
    "\n",
    "# add date that 1st bulk mailing was dropped off at USPS\n",
    "redcap_extract['survey_mailing_date_1'] = pd.to_datetime('11/13/2019')\n",
    "\n",
    "# NOTE: may want to add some additional items from Internal REDCap\n",
    "useful_redcap_cols = ['study_id', 'survey_language_sent', 'hipaa_sent_yn', 'survey_completed_dt', \n",
    "                      'consent_received_dt', 'survery_completed_method',  'hipaa_received_dt', \n",
    "                      'caregiver_name', 'consent_mailed_dt', 'gift_card_type', 'opt_out_pat_dt',\n",
    "                      'opted_out_patient_reasons', 'opted_out_patient_other',\n",
    "                      'opted_out_patient_transcription', 'survey_mailing_date_1']\n",
    "\n",
    "clean_redcap_extract = redcap_extract.filter(useful_redcap_cols).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 Mailing: merge mailing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the 4 mailing lists sent to mail vendor and combine \n",
    "all_mailings = [round_2_mailing_a, round_2_mailing_b, round_2_mailing_c, round_2_mailing_d]\n",
    "all_round_2_mailings = pd.concat(all_mailings)\n",
    "\n",
    "# add date that 2nd bulk mailing was dropped off at USPS \n",
    "all_round_2_mailings['survey_mailing_date_2'] = pd.to_datetime('2/18/2020')\n",
    "\n",
    "# only take necessary columns\n",
    "useful_round_2_mailing_cols = ['study_id', 'survey_mailing_date_2']\n",
    "clean_round_2_mailings = all_round_2_mailings.filter(useful_round_2_mailing_cols).copy()\n",
    "\n",
    "# convert study_id to string\n",
    "clean_round_2_mailings = clean_round_2_mailings.astype({'study_id': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarity Extract: calculate age and convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to datetime data type\n",
    "clarity_extract['birth_date'] = pd.to_datetime(clarity_extract['birth_date'])\n",
    "clarity_extract['death_date'] = pd.to_datetime(clarity_extract['death_date'])\n",
    "\n",
    "# convert int to string for consistency\n",
    "clarity_extract['study_id'] = clarity_extract['study_id'].astype(str)\n",
    "\n",
    "# calculate age from today's date, unless patient is dead (then use deceased date)\n",
    "# NOTE: may want to make this more precise and account for months \n",
    "def calculate_age(dob, dod): \n",
    "    today = date.today()\n",
    "    if dod is pd.NaT: \n",
    "        return today.year - dob.year\n",
    "    else: \n",
    "        return dod.year - dob.year\n",
    "\n",
    "clarity_extract['age'] = clarity_extract.apply(lambda x: calculate_age(x['birth_date'], x['death_date']), axis=1)\n",
    "\n",
    "\n",
    "# TODO: ask Rick for marital status, religion\n",
    "useful_clarity_cols = ['study_id', 'sex', 'race', 'ethnicity', 'age', 'spoken_language', 'written_language']\n",
    "\n",
    "clean_clarity_extract = clarity_extract.filter(useful_clarity_cols).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join REDCap Extract, REDCap Log, and Clarity Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of patients who received recruitment attempts:  892\n"
     ]
    }
   ],
   "source": [
    "# merge round 1 / 2 / 3 calls so there is 1 patient per line with all call data in columns \n",
    "clean_1_2_calls = pd.merge(clean_round_1, clean_round_2, how='left', on='study_id')\n",
    "clean_all_calls = pd.merge(clean_1_2_calls, clean_round_3, how='left', on='study_id')\n",
    "\n",
    "# join patients who were called with patients who returned surveys (source: REDCap report / extract)\n",
    "calls_and_redcap = pd.merge(clean_all_calls, clean_redcap_extract, how='outer', on='study_id')\n",
    "\n",
    "# add 2nd round mailing dates\n",
    "calls_and_redcap_and_mailing = pd.merge(calls_and_redcap, clean_round_2_mailings, how='left', on='study_id')\n",
    "\n",
    "# get full clarity data on patients who were called and / or returned surveys \n",
    "all_recruit = pd.merge(calls_and_redcap_and_mailing, clean_clarity_extract, how='left', on='study_id')\n",
    "\n",
    "print(\"Total # of patients who received recruitment attempts: \", all_recruit.shape[0])\n",
    "#print(\"\\n\\nColumns: \", all_recruit.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id                                   object\n",
      "caller_username_1                          object\n",
      "call_timestamp_1                   datetime64[ns]\n",
      "call_date_1                        datetime64[ns]\n",
      "call_output_1                              object\n",
      "call_notes_1                               object\n",
      "call_verbal_consent_1                      object\n",
      "caller_username_2                          object\n",
      "call_timestamp_2                   datetime64[ns]\n",
      "call_date_2                        datetime64[ns]\n",
      "call_output_2                              object\n",
      "call_notes_2                               object\n",
      "call_verbal_consent_2                      object\n",
      "caller_username_3                          object\n",
      "call_timestamp_3                   datetime64[ns]\n",
      "call_date_3                        datetime64[ns]\n",
      "call_output_3                              object\n",
      "call_notes_3                               object\n",
      "call_verbal_consent_3                      object\n",
      "survey_language_sent                       object\n",
      "hipaa_sent_yn                              object\n",
      "survey_completed_dt                datetime64[ns]\n",
      "consent_received_dt                datetime64[ns]\n",
      "survery_completed_method                   object\n",
      "hipaa_received_dt                  datetime64[ns]\n",
      "caregiver_name                             object\n",
      "consent_mailed_dt                  datetime64[ns]\n",
      "gift_card_type                             object\n",
      "opt_out_pat_dt                             object\n",
      "opted_out_patient_reasons                  object\n",
      "opted_out_patient_other                    object\n",
      "opted_out_patient_transcription            object\n",
      "survey_mailing_date_1              datetime64[ns]\n",
      "survey_mailing_date_2              datetime64[ns]\n",
      "sex                                        object\n",
      "race                                       object\n",
      "ethnicity                                  object\n",
      "age                                       float64\n",
      "spoken_language                            object\n",
      "written_language                           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(all_recruit.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_recruit.to_csv('data/processed/UCI_recruitment_cohort.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
